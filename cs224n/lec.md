## Lec 1
在神经网络之前，理解世界采用wordNet来做，但其只能做到近义词选取，没法做到上下文理解

这引导了人们做了WordVec，然而one-hot是不行的，得想办法根据context来做语义的鉴定，此外，词在很多情况下还很不一样，语义的分析慢慢转化成了vectors，或者是embeddings

这个名字是因为我们把某个词嵌入到某个高维空间的点上，并从原点搞一条向量去指向它

虽然这并不能完全解决上下文不同的感知（很重要），但是在一定程度上可以发挥功效

### word2ec:
简单来说：
- 从大量文本开始做，并且我们一开始对于某个词可能是有一个向量，之后呢，遍历文本中的词，对于每个词来说，分别有词本身(center word c)和上下文(outside words o)
- 利用c和o的word vectors计算给定o之后，出现c的概率。以及给定c之后，出现o的概率。
- 不停地调整，使得这个概率能够变得更大。

对于o词和c词，计算一个条件变量，把他们累乘，作为一个likelihood总值。并且对每一个位置的词都得做这样的工作，计算浏览的是一个窗口中的值，该窗口中要做好最基本的上下文视察
- 每个词均有两种表示方式，作为o词和c词，他们有不同的表示方式
- 而负责管理这种表示方式的变量，就是我们模型的变量$\theta$了

对于这边的概率定义，也做的比较直觉
- 比较容易在一起出现的词，会有比较高的文本相似度，所以o词和c词的乘积将会作为分子
- 指数化使得它变成正数，同时变得更加平滑，本质上是一个softmax的例子
- 分母则是对于整个词汇表的遍历，由于我们是从c词出发，所以这边将会存放整个词汇表的点积
- 初始值直接用随机值就可以了

最后利用gradient descent方式得到最后的比较高的概率分布

求导的过程很简单，一眼就能看明白