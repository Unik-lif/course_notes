## Slice 1
People tend to have multilingul model to translate many different lanuages to another lanuage, instead of some seperate ones

Researchers find:
- Imporved efficiency: Translating into and out of rare languages works better if the model is also trained on more common languages
- Zero-shot machine translation: you trained English to French, you trained French to Spainish, you can translate from English to Spanish with no examples

Belarusian Examples: Less examples, more rely on other languages like Russian
- The mixture in between Russian/Belarusian 0.44- 0.46 looks like Ukarine!

Story => Representations of DL.
- How to find the "THOUGHTS" behind the languages
- 联觉信标

Handling such complex inputs requires representations => The power of deep learning lies in its ability to learn such representations automatically from "IDEAS" and "THOUGHTS"